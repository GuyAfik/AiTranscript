# ğŸ™ï¸ AiTranscript

Voice transcription tool with AI-powered cleanup capabilities. Transcribe audio from YouTube videos, uploaded files, or live recordings, with intelligent summarization and message refinement using local LLM or OpenAI GPT.

## âœ¨ Features

- **Multiple Input Methods**:
  - YouTube URL transcription
  - Audio file upload (mp3, wav, m4a, ogg, flac)
  - Live voice recording

- **Dual AI Processing Modes**:
  - **Summarize Mode**: Get clear, concise summaries of transcripts with key points extraction
  - **Refine Mode**: Transform voice recordings into well-structured, professional messages

- **Flexible AI Providers**:
  - **Local LLM (Default)**: Run AI models on your machine via Ollama - free, private, no API key needed
  - **OpenAI GPT**: Cloud-based option for GPT-4/3.5 models

- **Local Transcription**: Uses Whisper model locally for privacy and cost-effectiveness
- **User-Friendly Interface**: Built with Streamlit for an intuitive web experience

## ğŸ—ï¸ Architecture

AiTranscript follows a service-oriented architecture with clear separation of concerns:

- **Services Layer**: YouTube extraction, audio transcription, AI summarization
- **Utils Layer**: Input validation, file handling
- **UI Layer**: Reusable Streamlit components

For detailed architecture information, see [`plans/architecture.md`](plans/architecture.md).

## ğŸ“‹ Prerequisites

- **Python**: 3.11 or higher
- **FFmpeg**: Required for audio processing
- **Ollama**: For local LLM (recommended, free)
- **OpenAI API Key**: Optional, only if using OpenAI provider

### Installing FFmpeg

**macOS**:
```bash
brew install ffmpeg
```

**Ubuntu/Debian**:
```bash
sudo apt-get update
sudo apt-get install ffmpeg
```

**Windows**:
Download from [ffmpeg.org](https://ffmpeg.org/download.html) and add to PATH.

### Installing Ollama (for Local LLM)

**macOS/Linux**:
```bash
curl https://ollama.ai/install.sh | sh
```

**Windows**:
Download from [ollama.ai](https://ollama.ai/download)

**Pull a model**:
```bash
# Start Ollama
ollama serve

# Pull a model (in another terminal)
ollama pull llama2
# or
ollama pull mistral
```

## ğŸš€ Quick Start

### 1. Clone the Repository

```bash
git clone <repository-url>
cd AiTranscript
```

### 2. Install uv (if not already installed)

```bash
# macOS/Linux
curl -LsSf https://astral.sh/uv/install.sh | sh

# Windows
powershell -c "irm https://astral.sh/uv/install.ps1 | iex"
```

### 3. Create Virtual Environment and Install Dependencies

```bash
# Create virtual environment
uv venv

# Activate virtual environment
# On macOS/Linux:
source .venv/bin/activate
# On Windows:
.venv\Scripts\activate

# Install dependencies
uv pip install -e .
```

### 4. Configure Environment Variables

```bash
# Copy the example environment file
cp .env.example .env

# Edit .env if needed (optional - defaults work out of the box)
```

Environment variables (all optional):
- `AI_PROVIDER`: Choose 'local' (default) or 'openai'
- `LOCAL_MODEL`: Local model to use (default: llama2)
- `OPENAI_API_KEY`: Your OpenAI API key (only if using OpenAI)
- `OPENAI_MODEL`: OpenAI model to use (default: gpt-4-turbo-preview)
- `WHISPER_MODEL_SIZE`: Whisper model size (default: base)

### 5. Run the Application

```bash
streamlit run app.py
```

The application will open in your default browser at `http://localhost:8501`.

## ğŸ“¦ Project Structure

```
aitranscript/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ youtube_service.py       # YouTube transcript extraction
â”‚   â”‚   â”œâ”€â”€ audio_service.py         # Audio transcription (Whisper)
â”‚   â”‚   â”œâ”€â”€ transcription_service.py # Voice recording handling
â”‚   â”‚   â””â”€â”€ ai_service.py            # AI cleanup and summarization
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ validators.py            # Input validation utilities
â”‚   â”‚   â””â”€â”€ file_handler.py          # File operations and cleanup
â”‚   â””â”€â”€ ui/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ components.py            # Reusable UI components
â”œâ”€â”€ app.py                           # Main Streamlit application
â”œâ”€â”€ pyproject.toml                   # Project configuration
â”œâ”€â”€ .env.example                     # Environment variables template
â””â”€â”€ README.md                        # This file
```

## ğŸ”§ Configuration

### Whisper Model Sizes

Choose the appropriate model size based on your needs:

| Model  | Size  | RAM   | Speed    | Accuracy | Use Case           |
|--------|-------|-------|----------|----------|--------------------|
| tiny   | 39M   | ~1GB  | Fastest  | Lowest   | Quick drafts       |
| base   | 74M   | ~1GB  | Fast     | Good     | Default choice     |
| small  | 244M  | ~2GB  | Medium   | Better   | Quality focus      |
| medium | 769M  | ~5GB  | Slow     | High     | Professional       |
| large  | 1550M | ~10GB | Slowest  | Best     | Maximum accuracy   |

Set in `.env`:
```bash
WHISPER_MODEL_SIZE=base
```

### AI Provider Models

#### Local LLM Models (via Ollama) - Default & Recommended

| Model      | Size  | RAM   | Speed    | Quality   | Use Case           |
|------------|-------|-------|----------|-----------|-------------------|
| llama2     | 3.8GB | ~8GB  | Fast     | Good      | Default, balanced |
| llama3     | 4.7GB | ~8GB  | Fast     | Excellent | Best quality      |
| mistral    | 4.1GB | ~8GB  | Fast     | Very Good | Great alternative |
| phi        | 1.6GB | ~4GB  | Fastest  | Good      | Low resource      |
| codellama  | 3.8GB | ~8GB  | Fast     | Good      | Code-focused      |

**Advantages**:
- âœ… Free - no API costs
- âœ… Private - data stays on your machine
- âœ… No API key needed
- âœ… Works offline

**Setup**:
```bash
# Install Ollama
curl https://ollama.ai/install.sh | sh

# Pull a model
ollama pull llama2

# Run the app (Ollama will start automatically)
streamlit run app.py
```

#### OpenAI Models (Optional)

| Model            | Cost/1K tokens | Speed  | Quality   | Use Case         |
|------------------|----------------|--------|-----------|------------------|
| gpt-3.5-turbo    | $0.0015        | Fast   | Good      | Cost-effective   |
| gpt-4-turbo      | $0.01          | Medium | Excellent | High quality     |
| gpt-4            | $0.03          | Slow   | Best      | Premium quality  |

**Setup**:
```bash
# Set in .env
AI_PROVIDER=openai
OPENAI_API_KEY=your-api-key-here
OPENAI_MODEL=gpt-4-turbo-preview
```

## ğŸ› ï¸ Development

### Using uv for Package Management

```bash
# Add a new dependency
uv pip install package-name

# Add a development dependency
uv pip install --dev package-name

# Update dependencies
uv pip install --upgrade package-name

# Sync dependencies from pyproject.toml
uv pip sync
```

### Running Tests (Coming Soon)

```bash
pytest
```

### Code Formatting

```bash
# Format code with black
black .

# Lint with ruff
ruff check .
```

## ğŸ“ Usage

### Processing Modes

**Summarize Mode** - Get clear summaries of content:
- Perfect for YouTube videos, podcasts, or long recordings
- Extracts key points automatically
- Choose from concise, detailed, or bullet-point styles

**Refine Mode** - Transform your voice into professional messages:
- Record what you want to say naturally
- AI refines it into a clear, well-structured message
- Choose tone: professional, friendly, formal, or casual
- Optionally specify recipient context for better refinement

### Step-by-Step Guide

1. **Configure Settings** (in sidebar):
   - Select AI provider (OpenAI or Gemini)
   - Choose your model
   - Enter your API key
   - Select processing mode (Summarize or Refine)
   - Configure mode-specific options

2. **Choose Input Method**:

   **YouTube Transcription**:
   - Navigate to the "YouTube" tab
   - Paste a YouTube URL
   - Click "Get Transcript"
   - View transcript and AI-processed result

   **File Upload**:
   - Navigate to the "Upload File" tab
   - Upload an audio file (mp3, wav, m4a, ogg, flac)
   - Click "Transcribe File"
   - View transcript and AI-processed result

   **Voice Recording**:
   - Navigate to the "Record Audio" tab
   - Click the microphone button to start/stop recording
   - Click "Transcribe Recording"
   - View transcript and AI-processed result

3. **Download Results**:
   - Use the download buttons to save your transcript and AI-processed output

## ğŸ”’ Privacy & Security

- **Local Transcription**: Audio is transcribed locally using Whisper (no data sent to external services)
- **No Data Storage**: Transcripts are not stored permanently
- **Temporary Files**: Automatically cleaned up after processing
- **API Key Security**: Store your OpenAI API key securely in `.env` (never commit to version control)

## ğŸš¢ Deployment

### Streamlit Cloud (Recommended for MVP)

1. Push your code to GitHub
2. Sign up for [Streamlit Cloud](https://streamlit.io/cloud)
3. Connect your GitHub repository
4. Add your `OPENAI_API_KEY` in the Secrets section
5. Deploy!

### Docker (Coming Soon)

```bash
docker-compose up
```

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- [OpenAI Whisper](https://github.com/openai/whisper) for local transcription
- [Ollama](https://ollama.ai/) for local LLM support
- [Streamlit](https://streamlit.io/) for the web framework
- [OpenAI](https://openai.com/) for GPT API
- [Google Gemini](https://ai.google.dev/) for Gemini API

## ğŸ“ Support

For issues and questions, please open an issue on GitHub.

---

**Built with â¤ï¸ using Streamlit, Whisper, OpenAI GPT, and Google Gemini**
